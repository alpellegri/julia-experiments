{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildNetworkArchitectureWithOneHiddenSigmoids (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract Layer\n",
    "\n",
    "# append vertical\n",
    "function appendColumnOfOnes(a::Array{Float64,2})\n",
    "  vcat(a,ones(1,size(a ,2)))\n",
    "end\n",
    "\n",
    "# params: horizontal\n",
    "# input: vertical\n",
    "function sigmoidNeuronTransformFunction(params, input)\n",
    "  return 1.0 ./ (1.0 .+ exp(-params * appendColumnOfOnes(input)))\n",
    "end\n",
    "\n",
    "function linearNeuronTransformFunction(params, input)\n",
    "  return params * appendColumnOfOnes(input)\n",
    "end\n",
    "\n",
    "function exponentialNormalizer(params, input)\n",
    "  denominator = sum(exp(input),1)\n",
    "  return exp(input) ./ denominator\n",
    "end\n",
    "\n",
    "type FullyConnectedComputingLayer <: Layer\n",
    "  inputSize::Int64\n",
    "  numberOfNeurons::Int64\n",
    "  parameters::Array{Float64,2}\n",
    "  transform::Function\n",
    "  derivative::Function # derivative added here\n",
    "\n",
    "  function FullyConnectedComputingLayer(inputSize, numberOfNeurons, transform::Function, derivative::Function)\n",
    "    parameters = randn(numberOfNeurons, inputSize + 1)  * 0.1 # adding one param column for bias\n",
    "    return new(inputSize, numberOfNeurons, parameters, transform, derivative)\n",
    "  end\n",
    "end\n",
    "\n",
    "type SoftMaxLayer <: Layer\n",
    "  numberOfNeurons::Int64\n",
    "  parameters::Any\n",
    "  transform::Function\n",
    "\n",
    "  function SoftMaxLayer(numberOfNeurons)\n",
    "    return new(numberOfNeurons, [], exponentialNormalizer)\n",
    "  end\n",
    "end\n",
    "\n",
    "type NetworkArchitecture\n",
    "  layers::Array{Layer}\n",
    "  function NetworkArchitecture(firstLayer::Layer)\n",
    "    return new([firstLayer])\n",
    "  end\n",
    "end\n",
    "\n",
    "function addSoftMaxLayer(architecture::NetworkArchitecture)\n",
    " lastNetworkLayer = architecture.layers[end]\n",
    " numberOfNeurons = lastNetworkLayer.numberOfNeurons\n",
    " softMaxLayer = SoftMaxLayer(numberOfNeurons)\n",
    " push!(architecture.layers, softMaxLayer)\n",
    "end\n",
    "\n",
    "function addFullyConnectedSigmoidLayer(arch::NetworkArchitecture, numberOfNeurons::Int64)\n",
    " lastNetworkLayer = arch.layers[end]\n",
    " inputSize = lastNetworkLayer.numberOfNeurons\n",
    " #derivative added\n",
    " sigmoidLayer = FullyConnectedComputingLayer(inputSize, numberOfNeurons, sigmoidNeuronTransformFunction, x -> x .* (1 - x))\n",
    " push!(arch.layers, sigmoidLayer)\n",
    "end\n",
    "\n",
    "function addFullyConnectedLinearLayer(architecture::NetworkArchitecture, numberOfNeurons::Int64)\n",
    " lastNetworkLayer = architecture.layers[end]\n",
    " inputSize = lastNetworkLayer.numberOfNeurons\n",
    " linearLayer = FullyConnectedComputingLayer(inputSize, numberOfNeurons, linearNeuronTransformFunction, x -> 1)\n",
    " push!(architecture.layers, linearLayer)\n",
    "end\n",
    "\n",
    "function infer(architecture::NetworkArchitecture, input)\n",
    "  currentResult = input\n",
    "  for i in 1:length(architecture.layers)\n",
    "     layer = architecture.layers[i]\n",
    "     currentResult = layer.transform(layer.parameters, currentResult)\n",
    "  end\n",
    "  return currentResult\n",
    "end\n",
    "\n",
    "function crossEntropyError(architecture::NetworkArchitecture, input, labels)\n",
    " probabilitiesSparseMatrix = infer(architecture, input) .* labels\n",
    " probabilities = sum(probabilitiesSparseMatrix , 1)\n",
    " return -mean(log(probabilities))\n",
    "end\n",
    "\n",
    "type BackPropagationBatchLearningUnit\n",
    "  networkArchitecture::NetworkArchitecture\n",
    "  dataBatch::Array{Float64,2}\n",
    "  labels::AbstractSparseMatrix\n",
    "  outputs::Array{Array{Float64,2}} # outputs remembered now\n",
    "  deltas::Array{Array{Float64,2}} # deltas kept here\n",
    "\n",
    "  function BackPropagationBatchLearningUnit(arch::NetworkArchitecture, dataBatch::Array{Float64,2}, labels::AbstractSparseMatrix)\n",
    "     outputs = [ zeros(l.numberOfNeurons, size(dataBatch,2)) for l in arch.layers ]\n",
    "     deltas = [ zeros(l.numberOfNeurons, size(dataBatch,2)) for l in arch.layers ]\n",
    "     return new(arch, dataBatch, labels, outputs, deltas)\n",
    "  end\n",
    "end\n",
    "\n",
    "function forwardPass!(learningUnit::BackPropagationBatchLearningUnit)\n",
    "  currentResult = learningUnit.dataBatch\n",
    "  for i in 1:length(learningUnit.networkArchitecture.layers)\n",
    "     layer = learningUnit.networkArchitecture.layers[i]\n",
    "     currentResult = layer.transform(layer.parameters, currentResult)\n",
    "     learningUnit.outputs[i]  = currentResult\n",
    "  end\n",
    "end\n",
    "\n",
    "function backwardPass!(learningUnit::BackPropagationBatchLearningUnit)\n",
    "\n",
    "  layer = learningUnit.networkArchitecture.layers[end-1]\n",
    "  learningUnit.deltas[end-1]  = layer.derivative(learningUnit.outputs[end-1]) .*  (learningUnit.outputs[end] - learningUnit.labels)\n",
    "\n",
    "  for i in 2:(length(learningUnit.networkArchitecture.layers) - 1)\n",
    "      higherLayer = learningUnit.networkArchitecture.layers[end - i + 1]\n",
    "      currentLayer = learningUnit.networkArchitecture.layers[end - i]\n",
    "      learningUnit.deltas[end-i] = currentLayer.derivative(learningUnit.outputs[end-i]) .* (transpose(higherLayer.parameters[:,(1:end-1)]) * learningUnit.deltas[end - i + 1])\n",
    "  end\n",
    "end\n",
    "\n",
    "function updateParameters!(unit::BackPropagationBatchLearningUnit, learningRate)\n",
    "  forwardPass!(unit)\n",
    "  backwardPass!(unit)\n",
    "  derivativeW= (unit.deltas[1] * transpose(unit.dataBatch)) / size(unit.dataBatch,2);\n",
    "  unit.networkArchitecture.layers[1].parameters[:,1:(end-1)] = unit.networkArchitecture.layers[1].parameters[:,1:(end-1)] - learningRate * derivativeW;\n",
    "  derivativeB = mean(unit.deltas[1],2);\n",
    "  unit.networkArchitecture.layers[1].parameters[:,end] =  unit.networkArchitecture.layers[1].parameters[:,end] - learningRate * derivativeB;\n",
    "  for i in 2:(length(unit.networkArchitecture.layers) - 1)\n",
    "    derivativeW = (unit.deltas[i] * transpose(unit.outputs[i-1])) / size(unit.dataBatch,2);\n",
    "    unit.networkArchitecture.layers[i].parameters[:,1:(end-1)] = unit.networkArchitecture.layers[i].parameters[:,1:(end-1)] - learningRate * derivativeW;\n",
    "    derivativeB = mean(unit.deltas[i],2);\n",
    "    unit.networkArchitecture.layers[i].parameters[:,end] =  unit.networkArchitecture.layers[i].parameters[:,end] - learningRate * derivativeB;\n",
    "  end\n",
    "end\n",
    "\n",
    "# helper to build SoftMax architecture\n",
    "function buildNetworkArchitectureSoftMax(sizes)\n",
    "  firstLayer = FullyConnectedComputingLayer(sizes[1], sizes[2], linearNeuronTransformFunction, x -> 1);\n",
    "  architecture = NetworkArchitecture(firstLayer);\n",
    "  addSoftMaxLayer(architecture)\n",
    "  return(architecture)\n",
    "end\n",
    "\n",
    "# helper to build an architecture with hidden sigmoid layers\n",
    "function buildNetworkArchitectureWithOneHiddenSigmoids(sizes)\n",
    "  firstLayer = FullyConnectedComputingLayer(sizes[1], sizes[2], sigmoidNeuronTransformFunction, x -> x .* (1 - x));\n",
    "  architecture = NetworkArchitecture(firstLayer);\n",
    "  for i in 3:(length(sizes)-1)\n",
    "    addFullyConnectedSigmoidLayer(architecture, sizes[i]);\n",
    "  end\n",
    "  addFullyConnectedLinearLayer(architecture, sizes[end]);\n",
    "  addSoftMaxLayer(architecture)\n",
    "  return(architecture)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching FullyConnectedComputingLayer(::Int64, ::Int64, ::#sigmoidNeuronTransformFunction)\u001b[0m\nClosest candidates are:\n  FullyConnectedComputingLayer(::Any, ::Any, ::Function, \u001b[1m\u001b[31m::Function\u001b[0m) at In[1]:31\n  FullyConnectedComputingLayer{T}(::Any) at sysimg.jl:53\u001b[0m",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching FullyConnectedComputingLayer(::Int64, ::Int64, ::#sigmoidNeuronTransformFunction)\u001b[0m\nClosest candidates are:\n  FullyConnectedComputingLayer(::Any, ::Any, ::Function, \u001b[1m\u001b[31m::Function\u001b[0m) at In[1]:31\n  FullyConnectedComputingLayer{T}(::Any) at sysimg.jl:53\u001b[0m",
      ""
     ]
    }
   ],
   "source": [
    "firstLayer = FullyConnectedComputingLayer(784, 100, sigmoidNeuronTransformFunction);\n",
    "architecture = NetworkArchitecture(firstLayer);\n",
    "addFullyConnectedSigmoidLayer(architecture, 50);\n",
    "addFullyConnectedSigmoidLayer(architecture, 30);\n",
    "addFullyConnectedLinearLayer(architecture, 9);\n",
    "addSoftMaxLayer(architecture);\n",
    "\n",
    "outputVector = infer(architecture , randn(784,1)) # we try a single vector now\n",
    "sum(outputVector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching FullyConnectedComputingLayer(::Int64, ::Int64, ::#sigmoidNeuronTransformFunction)\u001b[0m\nClosest candidates are:\n  FullyConnectedComputingLayer(::Any, ::Any, ::Function, \u001b[1m\u001b[31m::Function\u001b[0m) at In[1]:31\n  FullyConnectedComputingLayer{T}(::Any) at sysimg.jl:53\u001b[0m",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching FullyConnectedComputingLayer(::Int64, ::Int64, ::#sigmoidNeuronTransformFunction)\u001b[0m\nClosest candidates are:\n  FullyConnectedComputingLayer(::Any, ::Any, ::Function, \u001b[1m\u001b[31m::Function\u001b[0m) at In[1]:31\n  FullyConnectedComputingLayer{T}(::Any) at sysimg.jl:53\u001b[0m",
      ""
     ]
    }
   ],
   "source": [
    "firstLayer = FullyConnectedComputingLayer(784, 100, sigmoidNeuronTransformFunction)\n",
    "architecture = NetworkArchitecture(firstLayer)\n",
    "addFullyConnectedSigmoidLayer(architecture, 50)\n",
    "addFullyConnectedSigmoidLayer(architecture, 10)\n",
    "addFullyConnectedSigmoidLayer(architecture, 5)\n",
    "infer(architecture , randn(784,1)) # we try a single vector now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x -> x .* (1 - x))(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: x not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: x not defined",
      ""
     ]
    }
   ],
   "source": [
    "x(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×1 Array{Float64,2}:\n",
       " 0.419535\n",
       " 0.401596\n",
       " 0.538738\n",
       " 0.604009\n",
       " 0.513639"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstLayer = FullyConnectedComputingLayer(784, 100, sigmoidNeuronTransformFunction, x -> x .* (1 - x))\n",
    "architecture = NetworkArchitecture(firstLayer)\n",
    "addFullyConnectedSigmoidLayer(architecture, 50)\n",
    "addFullyConnectedSigmoidLayer(architecture, 10)\n",
    "addFullyConnectedSigmoidLayer(architecture, 5)\n",
    "infer(architecture , randn(784,1)) # we try a single vector now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkArchitecture(Layer[FullyConnectedComputingLayer(4,128,[-0.0404926 -0.0428208 … 0.097584 -0.00302284; 0.094352 -0.0578039 … -0.00680203 0.177206; … ; 0.00127318 0.0742024 … -0.204414 0.0700946; -0.0794905 0.0466298 … -0.0695677 0.0198188],sigmoidNeuronTransformFunction,#11),FullyConnectedComputingLayer(128,20,[0.0503732 0.306608 … 0.0590196 0.0322511; -0.0874081 0.186486 … 0.0992642 -0.0672814; … ; 0.148788 0.0063104 … -0.0334107 0.0506905; -0.0909246 0.000661362 … 0.114618 0.0169652],sigmoidNeuronTransformFunction,#1),FullyConnectedComputingLayer(20,5,[-0.111164 0.18771 … -0.10185 0.0951952; 0.0516701 -0.155717 … 0.0280639 -0.13353; … ; 0.0142033 -0.0211878 … -0.145402 0.10406; 0.190332 -0.147278 … 0.146077 0.184218],linearNeuronTransformFunction,#3),SoftMaxLayer(5,Any[],exponentialNormalizer)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture = buildNetworkArchitectureWithOneHiddenSigmoids([4,128,20, 5]) # 50 neurons in a hidden layer now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
